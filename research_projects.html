<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
    <title>Document</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <meta content="" name="description">
    <meta content="" name="author">

    <script>
     WebFontConfig = {
       google: {
         families: ['Noto Sans:400,700', 'Playfair Display:700']
       },
       timeout: 2000
     };

     (function(d) {
       var wf = d.createElement('script'), s = d.scripts[0];
       wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js';
       wf.async = true;
       s.parentNode.insertBefore(wf, s);
     })(document);
    </script>

    <link href="vendor/simple-line-icons/simple-line-icons.min.css" rel="stylesheet" type="text/css">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css">

    <link href="css/animate.css" rel="stylesheet">
    <link href="vendor/swiper/css/swiper.min.css" rel="stylesheet" type="text/css">
    <link href="css/layout.css" rel="stylesheet" type="text/css">
    <link rel="shortcut icon" href="favicon.ico">
  </head>
  <body>

    

  
  

  <header class="header">
    <nav class="navbar" role="navigation">
      <div class="container">
        <div class="menu-container">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="toggle-icon"></span>
          </button>

          <div class="navbar-logo">
            <a class="navbar-logo-wrap" href="index.html">
              <h1> SVL </h1>
              <!-- <img class="navbar-logo-img" src="img/logo.png" alt="Acidus Logo"> -->
            </a>
          </div>
        </div>

        <div class="collapse navbar-collapse nav-collapse">
          <div class="menu-container">
            <ul class="navbar-nav navbar-nav-right">
              <li class="nav-item">
                  <a class="nav-item-child "
                     href="index.html">Home</a>
              </li>
              <li class="nav-item">
                  <a class="nav-item-child "
                     href="people.html">People</a>
              </li>
              <li class="nav-item">
                  <a class="nav-item-child  active "
                     href="research_projects.html">Research</a>
              </li>
              <li class="nav-item">
                  <a class="nav-item-child "
                     href="publication.html">Publications</a>
              </li>
              <li class="nav-item">
                  <a class="nav-item-child "
                     href="resources.html">Resources</a>
              </li>
              <li class="nav-item">
                  <a class="nav-item-child "
                     href="teaching.html">Teaching</a>
              </li>
<!--               <li class="nav-item">
                <a class="nav-item-child "
                   href="contact.html">Contact</a>
              </li> -->
            </ul>
          </div>
        </div>
      </div>
    </nav>
  </header>



  <!-- Side Nav -->
  <div class="bg-color-sky-light section-seperator">
    <div class="content container">
      <div class="row">
        <div id="sidebar" class="sidebar-wrap col-md-4 col-xs-10 sm-margin-b-30">
          <ul class="sidebar list-unstyled sidenav-position">
            <li class="side-nav"><a href="#Highlight">Highlighted Projects</a></li>
            <li class="side-nav"><a href="#Vision">Vision & Language</a></li>
            <li class="side-nav"><a href="#2D">2D Image Understanding</a></li>
            <li class="side-nav"><a href="#3D">3D Scene and Object Understanding</a></li>
            <li class="side-nav"><a href="#Robotics">Robotics</a></li>
            <li class="side-nav"><a href="#Reinforcement">Reinforcement Learning</a></li>
            <li class="side-nav"><a href="#Video">Video and Activity Understanding</a></li>
            <li class="side-nav"><a href="#Learning">Learning Theory</a></li>
            <li class="side-nav"><a href="#Societal">Societal Understanding</a></li>
            <li class="side-nav"><a href="#Visual">Visual Reasoning</a></li>
            <li class="side-nav"><a href="#Data">Data and Label Generation</a></li>
            <li class="side-nav"><a href="#Human">Human Perception</a></li>
          </ul>
        </div>

        
        <div id="content" class="col-md-8 col-xs-12 sm-margin-b-30">
          <!-- Highlighted Projects -->
          <h2 class="top-seprates" id="Human">Highlighted Projects</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">JackRabbot</p>
              <p>Our work at the SVL is making practical a new generation of autonomous agents that can operate safely alongside humans in dynamic crowded environments such as terminals, malls, or campuses. The Stanford “Jackrabbot”, which takes it name from the nimble yet shy Jackrabbit, is a self-navigating automated electric delivery cart capable of carrying small payloads.</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/jackrabbot/">Link</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/projects/jackrabbot/cvgl_files/quartz_jr.mp4">Video</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</p>
              <p>Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: global video classification, trimmed activity classification and activity detection.</p>
              <p>Check out our <a class="link-sytle" href="http://activity-net.org/">Link</a> and <a class="link-sytle" href="http://www.niebles.net/images/anet_cvpr15b.jpg">Video</a></p>
            </li>
 
             <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">AI-Assisted Care</p>
              <p>The Partnership in AI-Assisted Care (PAC) is an interdisciplinary collaboration between the School of Medicine and the Computer Science department focusing on cutting edge computer vision and machine learning technologies to solve some of healthcare's most important problems.</p>
              <p>Check out our <a class="link-sytle" href="https://aicare.stanford.edu/">Link</a> and <a class="link-sytle" href="https://aicare.stanford.edu/projects/hand_hygiene/video/dispenser.mp4">Video</a></p>
            </li>

             <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Genome</p>
              <p>Short Description: To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. Visual Genome is a new dataset to connect dense, structured image concepts to language.</p>
              <p>Check out our <a class="link-sytle" href="http://visualgenome.org/">Link</a> and <a class="link-sytle" href="http://visualgenome.org/static/images/front-page/interconnected_images.png">Image</a></p>
            </li>

             <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ObjectNet3D</p>
              <p>ObjectNet3D is a large scale database for 3D object recognition. With 90,127 images, 201,888 objects in these images and 44,147 3D shapes, ObjectNet3D offers a powerful tool for learning to recognize 3D pose and 3D shape of objects from 2D images.</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/objectnet3d/">Link</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/research/ObjectNet3D">Video</a></p>
            </li>     

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ImageNet Challenge</p>
              <p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in computer vision across a wider variety of objects -- taking advantage of the quite expensive labeling efforts.</p>
              <p>Check out our <a class="link-sytle" href="http://www.image-net.org/challenges/LSVRC/">Link</a> and <a class="link-sytle" href="http://vision.stanford.edu/ilsvrc_proj.jpg">Image</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ImageNet</p>
              <p>ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. As the largest labelled image data set at the time of its release in 2009, ImageNet has helped to pave the way for today’s advances in visual recognition.</p>
              <p>Check out our <a class="link-sytle" href="http://www.image-net.org/">Link</a> and <a class="link-sytle" href="http://vision.stanford.edu/imagenet_proj.png.jpg">Image</a></p>
            </li>               
          </ul>
          <!-- end of Highlighted Projects -->  


          <!-- Vision and Language -->
          <h2 id="Vision" class="jumptarget">Visual Reasoning (visual question answering)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">A Hierarchical Approach for Generating Descriptive Image Paragraphs</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/ranjaykrishna/im2p/index.html">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/krause2017cvpr.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</p>
              <p>Check out our <a class="link-sytle" href="https://visualgenome.org/">Project Page</a>, <a class="link-sytle" href="http://vision.stanford.edu/pdf/visualgenome.pdf">Paper</a>, <a class="link-sytle" href="http://visualgenome.org/api/v0/api_home.html">Data</a>, <a class="link-sytle" href="https://github.com/ranjaykrishna/visual_genome_python_driver">API, <a class="link-sytle" href="https://twitter.com/visualgenome">Twitter</a></a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Relationship Detection with Language Priors</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/ranjaykrishna/vrd/">Project Page</a> and <a class="link-sytle" http://vision.stanford.edu/pdf/lu2016eccv.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/karpathy/densecap/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/johnson2016cvpr.pdff">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval</p>
              <p>Check out our <a class="link-sytle" href="http://nlp.stanford.edu/pubs/schuster-krishna-chang-feifei-manning-vl15.pdf">Project Page</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Love Thy Neighbors: Image Annotation by Exploiting Image Metadata</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/jcjohns/papers/iccv15/JohnsonICCV2015.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Image Retrieval using Scene Graphs</p>
              <p>Check out our<a class="link-sytle" href="http://cs.stanford.edu/people/jcjohns/papers/iccv15/JohnsonICCV2015.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Deep Fragment Embeddings for Bidirectional Image-Sentence Mapping</p>
              <p>Check out our <a class="link-sytle" href="http://feedbacknet.stanford.edu/">Project Page</a> and <a class="link-sytle" href="http://cs.stanford.edu/people/karpathy/nips2014.pdf">Paper</a></p>
            </li>
          </ul>
          <!-- End of Vision and Language -->

          <!-- 2D Image Understanding -->
          <h2 class="top-seprates jumptarget" id="2D">2D Image Understanding (segmentation, detection, depth incorporation, image matching)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">What's the point: Semantic segmentation with point supervision</p>
              <p>Check out our<a class="link-sytle" href="http://vision.stanford.edu/publications.html#"> Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Recurrent Attention Models for Depth-Based Person Identification</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/haque2016cvpr.pdf">Project Page</a> and <a class="link-sytle" href="http://www.albert.cm/projects/ram_person_id/">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</p>
              <p>Check out out <a class="link-sytle" href="http://yuxng.github.io/xiang_wacv17.pdf">Paper</a>, <a class="link-sytle" href="http://arxiv.org/abs/1604.04693">arXiv</a>, <a class="link-sytle" href="http://yuxng.github.io/xiang_wacv17_tr.pdf">Technical Report</a>, <a class="link-sytle" href="http://www.cvlibs.net/datasets/kitti/eval_object_detail.php?result=5e17cbbabbf775d8cc376793168be49bd6f01608">KITTI Results</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Robust Single-View Instance Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/papers/held_icra16.pdf">Paper</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/held_icra16.bib">bibtex</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Deep View Morphing</p>
              <p>Check out our <a class="link-sytle" href="https://arxiv.org/abs/1703.02168">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Universal Correspondence Network</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/ucn">Project Page</a>, <a class="link-sytle" href="https://papers.nips.cc/paper/6487-universal-correspondence-network">Paper</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/choy2016universal.bib">bibtex</a></p>
            </li>           
          </ul>
          <!-- 2D Image Understanding -->

          <!-- 3D Scene and Object Understanding -->
          <h2 class="top-seprates jumptarget" id="3D">3D Scene and Object Understanding (3D shape recognition, 3D reconstruction)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Towards Viewpoint Invariant 3D Human Pose Estimation</p>
              <p>Check out our <a class="link-sytle" href="https://www.albert.cm/projects/viewpoint_3d_pose/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/haque2016eccv.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Free your Camera: 3D Indoor Scene Understanding from Arbitrary Camera Motion</p>
              <p>Check out our<a class="link-sytle" href="http://vision.stanford.edu/pdf/furlan13.pdf"> Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">3D Object Representations for Fine-Grained Categorization</p>
              <p>Check out our <a class="link-sytle" href="http://feedbacknet.stanford.edu/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/3drr13.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</p>
              <p>Check out our <a class="link-sytle" href="http://3dsemantics.stanford.edu/">Project Page</a>, <a class="link-sytle" href="https://arxiv.org/pdf/1702.01105.pdf">Paper</a> and <a class="Link-sytle" href="http://buildingparser.stanford.edu/images/2D-3D-S.txt"></a>bibtex</p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</p>
              <p>Check out our <a class="link-sytle" href="http://3d-r2n2.stanford.edu/">Project Page</a>, <a class="link-sytle" href="http://download.springer.com/static/pdf/315/chp%253A10.1007%252F978-3-319-46484-8_38.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46484-8_38&token2=exp=1488871345~acl=%2Fstatic%2Fpdf%2F315%2Fchp%25253A10.1007%25252F978-3-319-46484-8_38.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fchapter%252F10.1007%252F978-3-319-46484-8_38*~hmac=87c4725a5031e36a5ede8b411ccaef0d38be69b891589f900bbd384d51728f73">Paper</a> <a class="link-sytle" href="https://github.com/chrischoy/3D-R2N2">Code</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/choy20163d.bib">bibtex</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ObjectNet3D: A Large Scale Database for 3D Object Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/objectnet3d">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_eccv16.pdf">Paper</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_eccv16_tr.pdf">Technical Report</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/xiang_eccv16.bib">bibtex</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">3D Semantic Parsing of Large-Scale Indoor Spaces</p>
              <p>Check out our <a class="link-sytle" href="http://buildingparser.stanford.edu/">Project Page</a>, <a class="link-sytle" href="http://buildingparser.stanford.edu/images/3D_Semantic_Parsing.pdf">Paper</a>, <a class="link-sytle" href="http://buildingparser.stanford.edu/images/supp_mat.pdf">Technical Report</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/armeni_cvpr16.bib">bibtex</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ShapeNet: An Information-Rich 3D Model Repository</p>
              <p>Check out our <a class="link-sytle" href="http://dblp.uni-trier.de/rec/bibtex/journals/corr/ChangFGHHLSSSSX15">bibtex</a> and <a class="link-sytle" href="http://arxiv.org/pdf/1512.03012v1.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Indoor Scene Understanding with Geometric and Semantic Contexts</p>
              <p>Check out our <a class="link-sytle" href=" http://cvgl.stanford.edu/bibtex/choi_ijcv15.bib">Project Page</a> and <a class="link-sytle" href="http://link.springer.com/article/10.1007%2Fs11263-014-0779-4">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/C2F">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/Mottaghi15cvpr.pdf">Paper</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/mottaghi_cvpr15.bib">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/Mottaghi15cvpr-sm.pdf">Technical Report</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Data-Driven 3D Voxel Patterns for Object Category Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/3DVP">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf">Paper</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/xiang_cvpr15.bib">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp_tr.pdf">Technical Report</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild</p>
              <p>Check out our <a class="link-sytle" href="Project: http://cvgl.stanford.edu/projects/pascal3d.html">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_wacv14.pdf">Paper</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/xiang_wacv14.bib">bibtex</a></p>
            </li>             
          </ul>
          <!-- end of 3D Scene and Object Understanding -->          

          <!-- Robotics -->
          <h2 class="top-seprates jumptarget" id="Robotics">Robotics (tracking, prediction, and localization)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Tracking Millions of Humans in Crowded Space</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/alahi2017gcbcv2.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning to Predict Human Behaviour in Crowded Scenes</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/alahi2017gcbcv.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Unsupervised Camera Localization in Crowded Spaces</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/alahi2017icra.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Social LSTM: Human Trajectory Prediction in Crowded Spaces</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/CVPR16_N_LSTM.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Socially-aware Large-scale Crowd Forecasting</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/alahi14.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Social Scene Understanding: End-to-End Multi-Person Action Localization and Collective Activity Recognition</p>
              <p>Check out our <a class="link-sytle" href="https://arxiv.org/abs/1611.09078">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies</p>
              <p>Check out our <a class="link-sytle" href="https://arxiv.org/abs/1701.01909"> arXiv</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning Social Etiquette: Human Trajectory Prediction</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/uav_data/">Project Page</a>, <a class="link-sytle" href="http://web.stanford.edu/~alahi/downloads/ECCV16social.pdf">Paper</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/robicquet2016learning.bib">bibtex</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning to Track at 100 FPS with Deep Regression Networks</p>
              <p>Check out our <a class="link-sytle" href="arXiv: http://arxiv.org/abs/1604.01802">arXiv</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/held_rss16.bib">arXiv</a> and <a class="link-sytle" href="http://davheld.github.io/segmentation3D/segmentation3D.htmlf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning to Track: Online Multi-Object Tracking by Decision Making</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/MDP_tracking">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_iccv15.pdf">Paper</a>,<a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/xiang_iccv15.bib">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/xiang_iccv15_tr.pdf">Technical Report</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Combining 3D Shape, Color, and Motion for Robust Anytime Tracking</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/held_rss2014.bib">Technical Report</a></p>
            </li>             
          </ul>
          <!-- end of Robotics -->    

          <!-- Reinforcement Learning -->
          <h2 class="top-seprates jumptarget" id="Reinforcement">Reinforcement Learning (policy & deep reinforcement learning)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Semantic Planning using Deep Successor Representations</p></p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/zhu2017iccv.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Adversarially Robust Policy Learning through Active Construction of Physically-Plausible Perturbations</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/mandlekar2017iros.pdf">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/zhu2017icra.pdf">Paper</a></p>
            </li>            
          </ul>
          <!-- end of Reinforcement Learning  -->   

          <!-- Video and Activity Understanding -->
          <h2 class="top-seprates jumptarget" id="Video">Video and Activity Understanding (action recognition, video captioning)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Dense-Captioning Events in Videos</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/ranjaykrishna/densevid/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</p>
              <p>Check out our <a class="link-sytle" href="https://aicare.stanford.edu/projects/hand_hygiene/">Project Page</a> and <a class="link-sytle" href="https://arxiv.org/pdf/1708.00163.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos</p>
              <p>Check out our <a class="link-sytle" href="http://ai.stanford.edu/~dahuang/projects/vlrr/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/huang2017cvpr.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning to learn from noisy web videos</p>
              <p>Check out our <a class="link-sytle" http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">SST: Single-Stream Temporal Action Proposals</p>
              <p>Check out our <a class="link-sytle" href=" http://vision.stanford.edu/pdf/buch2017cvpr.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Unsupervised Learning of Long-Term Motion Dynamics for Videos</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/luo2017cvpr.pdf">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Connectionist Temporal Modeling for Weakly Supervised Action Labeling</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">End-to-end Learning of Action Detection from Frame Glimpses in Videos</p>
              <p>Check out our <a class="link-sytle" href="http://ai.stanford.edu/~syyeung/frameglimpses.html">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/yeung2016cvpr.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Detecting Events and Key Actors in Multi-person Videos</p>
              <p>Check out our <a class="link-sytle" href="http://basketballattention.appspot.com/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/ramanathan2016cvpr.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning temporal embeddings for complex video analysis</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/RamanathanICCV2015.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Joint person naming in videos and coreference resolution in text</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/vignesh14.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Video Event Understanding using Natural Language Descriptions</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/vigneshICCV13.pdf">Paper</a></p>
            </li>     

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Action Recognition with Exemplar Based 2.5D Graph Matching</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/YaoFei-Fei_ECCV12.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Watch-n-Patch: Unsupervised Learning of Actions and Relations</p>
              <p>Check out our <a class="link-sytle" href="https://arxiv.org/pdf/1603.03541.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten Actions</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/wu_icra16.bib">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/wu_icra16.pdf">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Forecasting by Imitating Dynamics in Natural Sequences</p>
              <p>Check out our <a class="link-sytle" href="http://ai.stanford.edu/~dahuang/papers/iccv17-vfid.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos</p>
              <p>Check out our <a class="link-sytle" href="https://www.dropbox.com/s/9n90etsu6jubiax/0144.pdf?dl=1">Paper</a></p>
            </li> 
<!-- 
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">SST: Single-Stream Temporal Action Proposals</p>
              <p>Check out our <a class="link-sytle" href="http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/">Data</a> and <a class="link-sytle" href="http://dx.doi.org/10.1016/j.imavis.2016.11.004">Paper</a></p>
            </li>  -->

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Sparse composition of body poses and atomic actions for human activity recognition in RGB-D videos</p>
              <p>Check out our <a class="link-sytle" href="http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/">Data</a> and <a class="link-sytle" href="http://dx.doi.org/10.1016/j.imavis.2016.11.004">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">DAPs: Deep Action Proposals for Action Understanding</p>
              <p>Check out our <a class="link-sytle" href="https://ivul.kaust.edu.sa/Pages/pub-Daps.aspx">Project Page</a>, <a class="link-sytle" href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_47">Paper</a>, <a class="link-sytle" href="https://drive.google.com/open?id=0B0ZXjo_p8lHBRGlnZ0Q1dFk2SUk">Video</a> and <a class="link-sytle" href=" https://github.com/escorciav/daps">Code</a></p>
            </li>                                                                                         

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Title Generation for User Generated Videos</p>
              <p>Check out our <a class="link-sytle" href="http://ug-video.com/">Project Page</a>, <a class="link-sytle" href="https://link.springer.com/chapter/10.1007/978-3-319-46475-6_38">Paper</a>, <a class="link-sytle" href="https://arxiv.org/abs/1608.07068">arXiv</a>,<a class="link-sytle" href="https://www.youtube.com/watch?v=KCTQ0vMsyDc">Video</a> and <a class="link-sytle" href=" http://ug-video.com/">Data</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos</p>
              <p>Check out our <a class="link-sytle" href="http://www.cabaf.net/temporalproposals/index.html">Project Page</a>, <a class="link-sytle" href="https://github.com/cabaf/sparseprop">Code</a> and <a class="link-sytle" href="https://doi.org/10.1109/CVPR.2016.211">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">A Hierarchical Pose-Based Approach to Complex Action Understanding Using Dictionaries of Actionlets and Motion Poselets</p>
              <p>Check out our <a class="link-sytle" href="http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/">Data</a>, <a class="link-sytle" href="https://arxiv.org/abs/1606.04992">arXiv</a> and <a class="link-sytle" href="https://doi.org/10.1109/CVPR.2016.218">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding Unsupervised Semantic Parsing of Video Collections</p>
              <p>Check out our <a class="link-sytle" href="http://activity-net.org/">Project Page</a>, <a class="link-sytle" href="https://github.com/activitynet/ActivityNet">Code</a>, <a class="link-sytle" href="http://activity-net.org/download.html">Data</a> and <a class="link-sytle" href="https://doi.org/10.1109/CVPR.2015.7298698">Paper</a></p>
            </li>                                                    

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Action Recognition by Hierarchical Mid-level Action Elements</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/lan_iccv2015.bib">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/tian2015.pdf">Paper</a></p>
            </li>   
          </ul>
          <!-- end of Video and Activity Understanding --> 

          <!-- Learning Theory -->
          <h2 class="top-seprates jumptarget" id="Learning">Learning Theory (metric learning, style transfer, domain adaptation)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Characterizing and Improving Stability in Neural Style Transfer</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Perceptual Losses for Real-time Style Transfer and Single Image Super-Resolution</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visualizing and Understanding Recurrent Networks</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Feedback Networks</p>
              <p>Check out our <a class="link-sytle" href="http://feedbacknet.stanford.edu/">Project Page</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/zamir2016feedback.bib">bibtex</a> and <a class="link-sytle" href="https://arxiv.org/pdf/1612.09508">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Learning Transferrable Representations for Unsupervised Domain Adaptation</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/bibtex/soner2016learning.bib">bibtex</a> and <a class="link-sytle" href="https://papers.nips.cc/paper/6360-learning-transferrable-representations-for-unsupervised-domain-adaptation.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Deep Metric Learning via Lifted Structured Feature Embedding</p>
              <p>Check out our <a class="link-sytle" href="http://cvgl.stanford.edu/projects/lifted_struct">Project Page</a>, <a class="link-sytle" href="https://github.com/rksltnl/Deep-Metric-Learning-CVPR16">Code</a>, <a class="link-sytle" href="http://cvgl.stanford.edu/papers/song_cvpr16_supp.pdf">Technical Report</a>, <a class="link-sytle" href="http://ai.stanford.edu/~hsong/bibs/Song-CVPR16.txt">bibtex</a> and <a class="link-sytle" href="http://cvgl.stanford.edu/papers/song_cvpr16.pdf">Paper</a></p>
            </li>   
           </ul> 
          <!-- end of Learning Theory -->    

          <!-- Societal Understanding -->
          <h2 class="top-seprates jumptarget" id="Societal">Societal Understanding (street view detection for census estimation)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Fine-Grained Car Detection for Visual Census Estimation</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/gebru2017aaai.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Vision-Based Classification of Developmental Disorders Using Eye-Movements</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/pusiol2016miccai.pdf">Paper</a></p>
            </li>            
          </ul>
          <!-- end of Societal Understanding -->   

          <!-- Visual Reasoning -->
          <h2 class="top-seprates jumptarget" id="Visual">Visual Reasoning (visual question answering)</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Inferring and Executing Programs for Visual Reasoning</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/jcjohns/iep">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/publications.html#">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Knowledge Acquisition for Visual Question Answering via Iterative Querying</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/zhu2017cvpr.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</p>
              <p>Check out our <a class="link-sytle" href="http://cs.stanford.edu/people/jcjohns/clevr/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/johnson2017cvpr.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual7W: Grounded Question Answering in Images</p>
              <p>Check out our <a class="link-sytle" href="http://web.stanford.edu/~yukez/visual7w/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/zhu2016cvpr.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Leveraging Video Descriptions to Learn Video Question Answering</p>
              <p>Check out our <a class="link-sytle" href="http://aliensunmin.github.io/project/video-language/index.html#VideoQA">Project Page</a>, <a class="link-sytle" href="http://ug-video.com/">Data</a>, <a class="link-sytle" href="https://arxiv.org/abs/1611.04021">arXiv</a> and <a class="link-sytle" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14906">Paper</a></p>
            </li>                                               
          </ul>
          <!-- end of Visual Reasoning -->   

          <!-- Data and Label Generation -->
          <h2 class="top-seprates jumptarget" id="Data">Data and Label Generation</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Scalable Annotation of Fine-Grained Objects Without Experts</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/gebru2017chi.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">A Glimpse Far into the Future: Understanding Long-term Crowd Worker Accuracy</p>
              <p>Check out our <a class="link-sytle" href=" http://cs.stanford.edu/people/ranjaykrishna/glimpse/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/hata2017cscw.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://arxiv.org/abs/1511.06789">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Embracing Error to Enable Rapid Crowdsourcing</p>
              <p>Check out our <a class="link-sytle" href="http://hci.stanford.edu/publications/2016/rsvp/rsvp.pdf">Paper</a></p>
            </li> 

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ImageNet Large Scale Visual Recognition Challenge</p>
              <p>Check out our <a class="link-sytle" href="http://image-net.org/challenges/LSVRC/">Project Page</a>, <a class="link-sytle" href="http://ai.stanford.edu/~olga/bibtex/ILSVRC15.bib">bibtex</a> and <a class="link-sytle" href="http://arxiv.org/abs/1409.0575">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">ImageNet: A Large-Scale Hierarchical Image Database</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/ImageNet_CVPR2009.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Scalable Multi-Label Annotation</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/chi2014-MultiLabel.pdf">Paper</a></p>
            </li>   

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Fine-Grained Crowdsourcing for Fine-Grained Recognition</p>
              <p>Check out our <a class="link-sytle" href="http://feedbacknet.stanford.edu/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/DengKrauseFei-Fei_CVPR2013.pdf">Paper</a></p>
            </li>                                                         
          </ul>
          <!-- end of Visual Reasoning -->  

          <!-- Human Perception -->
          <h2 class="top-seprates jumptarget" id="Human">Human Perception</h2>
          <ul class="row list-unstyled">
            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Human-object Interactions are More than the Sum of Their Parts</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/CC2016.pdf">Paper</a></p>
            </li>

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Typicality Sharpens Category Representations in Object-Selective Cortex</p>
              <p>Check out our <a class="link-sytle" href="http://vision.stanford.edu/pdf/iordan-etal-neuroimage-2016.pdf">Paper</a></p>
            </li>  

            <li class="col-md-11 col-xs-12">
              <p class="name-title article-seprates">Visual Categorization is Automatic and Obligatory: Evidence from a Stroop-like Paradigm</p>
              <p>Check out our <a class="link-sytle" href="http://feedbacknet.stanford.edu/">Project Page</a> and <a class="link-sytle" href="http://vision.stanford.edu/pdf/14.full.pdf">Paper</a></p>
            </li>                                                       
          </ul>
          <!-- end of Human Perception -->  
        </div>
      </div>
    </div>
  </div>



  <footer class="footer">
<!--   <div class="section-separator">
    <div class="content-md container">
      <div class="row">
        <div class="col-sm-2 sm-margin-b-30">
          <ul class="list-unstyled footer-list">
            <li class="footer-list-item"><a href="index.html">Home</a></li>
            <li class="footer-list-item"><a href="people.html">People</a></li>



          </ul>
        </div>
        <div class="col-sm-2 sm-margin-b-30">
          <ul class="list-unstyled footer-list">
            <li class="footer-list-item"><a href="research_projects.html">Research</a></li>
            <li class="footer-list-item"><a href="publication.html">Publications</a></li>
          </ul>
        </div>
        <div class="col-sm-3">
          <ul class="list-unstyled footer-list">
            <li class="footer-list-item"><a href="resources.html">Resources</a></li>
            <li class="footer-list-item"><a href="teaching.html">Teaching</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div> -->
  <div class="content-footer container">
    <div class="row">
      <div class="col-xs-6">
        <h2>Contact Us</h2>
         <p class="margin-b-0">Admin: Tin Tin Wisniewski 
         <br>Email: tintinyw at cs dot stanford dot edu 
         <br>Phone: (650) 723-3819 
         <br>Fax: (650) 725-1449 

        <!-- <img class="footer-logo" src="img/logo.png" alt="Acidus Logo"> -->
      </div>
      <div class="col-xs-6 text-right">
      <h2>-</h2>
        <p class="margin-b-0">
         <br>Computer Science Department 
         <br>Stanford University 
         <br>353 Serra Mall, Stanford, CA 94305-9025. 
        </p>
      </div>
    </div>
  </div>
</footer>

<a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>


  <!-- JAVASCRIPTS(Load javascripts at bottom, this will reduce page load time) -->
  <script src="vendor/jquery.min.js" type="text/javascript"></script>
  <script src="vendor/jquery-migrate.min.js" type="text/javascript"></script>
  <script src="vendor/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>

  <script src="vendor/jquery.easing.js" type="text/javascript"></script>
  <script src="vendor/jquery.back-to-top.js" type="text/javascript"></script>
  <script src="vendor/jquery.smooth-scroll.js" type="text/javascript"></script>
  <script src="vendor/jquery.wow.min.js" type="text/javascript"></script>
  <script src="vendor/swiper/js/swiper.jquery.min.js" type="text/javascript"></script>
  <script src="vendor/masonry/jquery.masonry.pkgd.min.js" type="text/javascript"></script>
  <script src="vendor/masonry/imagesloaded.pkgd.min.js" type="text/javascript"></script>

  <script src="js/layout.min.js" type="text/javascript"></script>
  <script src="js/components/wow.min.js" type="text/javascript"></script>
  <script src="js/components/swiper.min.js" type="text/javascript"></script>
  <script src="js/components/masonry.min.js" type="text/javascript"></script>
  
  <!-- <script src="js/sticky-sidebar.min.js" type="text/javascript"></script>

       <script type="text/javascript">
       var sidebar = new StickySidebar('#sidebar', {
       containerSelector: '#content',
       innerWrapperSelector: '.sidebar__inner',
       topSpacing: 20,
       bottomSpacing: 20
       });
       </script>
     -->



  </body>
</html>
